xgb模型
1.avg, max, missing_num  xgb_feature_v1_20170929
验证集：
[739]   eval-auc:0.826016       train-auc:0.950718
[820]   eval-auc:0.82624        train-auc:0.954262
[999]   eval-auc:0.825712       train-auc:0.961071

测试集：
[819]   train-auc:0.95029
线上：0.86558

[999]	train-auc:0.957313
线上：0.86748


xgb多模型
跑20个模型，然后求平均
验证集：0.83534626709

跑29个模型，然后求平均
验证集：0.835464212844

跑36个模型，然后求平均
验证集：0.835633174677

xgb多模型-线上  xgb_feature_v1_20170929
15个模型，然后求平均：0.86858
18个模型，然后求平均：0.86890
25个模型，然后求平均：0.86906
36个模型，然后求平均：0.86874

xbg+组合特征
验证集：
[696]   eval-auc:0.828088       train-auc:0.955805
[799]   eval-auc:0.828194       train-auc:0.960808
[950]   eval-auc:0.82903        train-auc:0.967083

train:
[799]	train-auc:0.955851
线上：0.86367
多加n轮使得validate最好
[9]	train-auc:0.967096
线上：0.86494
效果不好，不加这个特征

xgb+log组合特征
train:[899]	train-auc:0.957543
线上：0.86843（相比86748）


TODO:
1.xgb跑36个模型bagging的结果验证

2.验证组合feature,相乘后取log
0.001的提升

3.验证组合feature,相除
线上and验证集效果不好

4.类别不平衡问题，验证加入over-sample
验证集效果不好

5.加入large-SVM


单模型XGboost得到87041
单模型FM得到85940
模型融合
xgboost:0.65 + fm 0.35 = 0.87254  采用rank-ensemble得到0.87270
xgboost:0.75 + fm 0.25 = 0.87246


LR模型:
C=0.1, tol=1e-6, solver='liblinear', max_iter=1000, penalty='l1'
验证集: train 0.8790246935 test 0.815583586996

LR ensemble
gbdt dart goss

C=0.03, tol=1e-6, dual=False, solver='liblinear', max_iter=10000, penalty='l2', verbose=0, n_jobs=1
cailiming_gbdt_dart_goss_86383
rf太低0.78，所以去掉了

rank-ensemble:
xgb_ans['ovd_rate'] * 0.6 + other_tree_ans['ovd_rate'] * 0.1 + fm_ans['ovd_rate'] * 0.3
0.873416037305

dnn-result(validate):
lr=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0 epochs=1
0.772363

lr=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0 epochs=1 最后一层dropout 0.2
0.777371

加不同时间段特征
1.加半年的特征，融合
[1049]  train-auc:0.960887    AUC:87357

2.加最近三月的特征，融合
[950]  train-auc:0.959896     AUC:87017

2.加一个月的特征，融合
[1000]  train-auc:0.961411    AUC:0.86858